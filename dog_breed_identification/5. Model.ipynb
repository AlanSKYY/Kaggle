{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import init, gluon, nd, autograd, image\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data import vision\n",
    "from mxnet.gluon.model_zoo import vision as models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import h5py\n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Change the following to mx.cpu() if you don't have GPU in your computer.\n",
    "# To use different GPU, you can try \"ctx = mx.gpu(1)\", where 1 is the first GPU.\n",
    "ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir        = 'data'\n",
    "batch_size      = 128\n",
    "learning_rate   = 1e-3\n",
    "epochs          = 150\n",
    "lr_decay        = 0.95\n",
    "lr_decay2       = 0.8\n",
    "lr_period       = 100\n",
    "submit_fileName = 'pred.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20580/20580 [00:01<00:00, 10626.80it/s]\n"
     ]
    }
   ],
   "source": [
    "synset = list(pd.read_csv(os.path.join('.', data_dir, 'sample_submission.csv')).columns[1:])\n",
    "n = len(glob(os.path.join('.', data_dir, 'Images', '*', '*.jpg')))\n",
    "\n",
    "y = nd.zeros((n,))\n",
    "for i, file_name in tqdm(enumerate(glob(os.path.join('.', data_dir, 'Images', '*', '*.jpg'))), total=n):\n",
    "    y[i] = synset.index(file_name.split('/')[3][10:].lower())\n",
    "    nd.waitall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading features of Stanford dogs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20580, 4096)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [nd.load(os.path.join(data_dir, 'features_incep.nd'))[0], \\\n",
    "            nd.load(os.path.join(data_dir, 'features_res.nd'))[0]]\n",
    "features = nd.concat(*features, dim=1)\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading features of testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10357, 4096)\n"
     ]
    }
   ],
   "source": [
    "models   = ['incep', 'res']\n",
    "features_test = [nd.load(os.path.join(data_dir, 'features_test_%s.nd') % model)[0] for model in models]\n",
    "features_test = nd.concat(*features_test, dim=1)\n",
    "\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    net = nn.Sequential()\n",
    "    with net.name_scope():\n",
    "        net.add(nn.BatchNorm())\n",
    "        net.add(nn.Dense(1024))\n",
    "        net.add(nn.BatchNorm())\n",
    "        net.add(nn.Activation('relu'))\n",
    "#         net.add(nn.Dropout(0.5))\n",
    "        net.add(nn.Dense(512))\n",
    "        net.add(nn.BatchNorm())\n",
    "        net.add(nn.Activation('relu'))\n",
    "#         net.add(nn.Dropout(0.5))\n",
    "        net.add(nn.Dense(120))\n",
    "    net.initialize(ctx=ctx)\n",
    "    return net\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    return nd.mean(nd.argmax(output, axis=1) == labels).asscalar()\n",
    "\n",
    "\n",
    "def evaluate(net, data_iter):\n",
    "    loss, acc, n = 0., 0., 0.\n",
    "    steps = len(data_iter)\n",
    "    for data, label in data_iter:\n",
    "        data, label = data.as_in_context(ctx), label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        acc += accuracy(output, label)\n",
    "        loss += nd.mean(softmax_cross_entropy(output, label)).asscalar()\n",
    "    return loss/steps, acc/steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter_train       = gluon.data.DataLoader(gluon.data.ArrayDataset(features, y), batch_size, shuffle=True)\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "net                   = build_model()\n",
    "trainer               = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. loss: 0.5165, acc: 87.83%, val_loss 0.1367, val_acc 95.91%\n",
      "Epoch 11. loss: 0.0140, acc: 99.56%, val_loss 0.0122, val_acc 99.62%\n",
      "Epoch 21. loss: 0.0059, acc: 99.75%, val_loss 0.0046, val_acc 99.80%\n",
      "Epoch 31. loss: 0.0043, acc: 99.74%, val_loss 0.0035, val_acc 99.80%\n",
      "Epoch 41. loss: 0.0038, acc: 99.72%, val_loss 0.0029, val_acc 99.81%\n",
      "Epoch 51. loss: 0.0034, acc: 99.73%, val_loss 0.0027, val_acc 99.82%\n",
      "Epoch 61. loss: 0.0031, acc: 99.75%, val_loss 0.0027, val_acc 99.82%\n",
      "Epoch 71. loss: 0.0029, acc: 99.77%, val_loss 0.0026, val_acc 99.82%\n",
      "Epoch 81. loss: 0.0028, acc: 99.73%, val_loss 0.0026, val_acc 99.82%\n",
      "Epoch 91. loss: 0.0027, acc: 99.76%, val_loss 0.0026, val_acc 99.82%\n",
      "Epoch 101. loss: 0.0026, acc: 99.78%, val_loss 0.0026, val_acc 99.82%\n",
      "Epoch 111. loss: 0.0026, acc: 99.82%, val_loss 0.0026, val_acc 99.82%\n",
      "Epoch 121. loss: 0.0026, acc: 99.82%, val_loss 0.0026, val_acc 99.82%\n",
      "Epoch 131. loss: 0.0026, acc: 99.83%, val_loss 0.0026, val_acc 99.82%\n",
      "Epoch 141. loss: 0.0026, acc: 99.82%, val_loss 0.0026, val_acc 99.82%\n",
      "Epoch 150. loss: 0.0026, acc: 99.83%, val_loss 0.0026, val_acc 99.82%\n",
      "CPU times: user 9min 30s, sys: 1min 9s, total: 10min 39s\n",
      "Wall time: 6min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# https://github.com/yinglang/CIFAR10_mxnet/blob/master/CIFAR10_train.md\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    if epoch <= lr_period:\n",
    "        trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "    else:\n",
    "        trainer.set_learning_rate(trainer.learning_rate * lr_decay2)\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    steps = len(data_iter_train)\n",
    "    for data, label in data_iter_train:\n",
    "        data, label = data.as_in_context(ctx), label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "        train_loss += nd.mean(loss).asscalar()\n",
    "        train_acc += accuracy(output, label)\n",
    "\n",
    "    val_loss, val_acc = evaluate(net, data_iter_train)\n",
    " \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch %d. loss: %.4f, acc: %.2f%%, val_loss %.4f, val_acc %.2f%%\" % (\n",
    "            epoch+1, train_loss/steps, train_acc/steps*100, val_loss, val_acc*100))\n",
    "\n",
    "print(\"Epoch %d. loss: %.4f, acc: %.2f%%, val_loss %.4f, val_acc %.2f%%\" % (\n",
    "    epoch+1, train_loss/steps, train_acc/steps*100, val_loss, val_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the trained network on the testing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = nd.softmax(net(nd.array(features_test).as_in_context(ctx))).asnumpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputing submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.read_csv(os.path.join('.', data_dir, 'sample_submission.csv'))\n",
    "\n",
    "for i, c in enumerate(df_pred.columns[1:]):\n",
    "    df_pred[c] = output[:,i]\n",
    "\n",
    "df_pred.to_csv(os.path.join('.', data_dir, submit_fileName), index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
